import os
import asyncio
import gradio as gr
import openai
from dotenv import load_dotenv
import httpx
from models_list import MODELS


# ---------- 0. è®€ç’°å¢ƒè®Šæ•¸ ----------
load_dotenv()
"""
OR_KEY = os.getenv("OPENROUTER_API_KEY")
OA_KEY = os.getenv("OPENAI_API_KEY")
PP_KEY = os.getenv("PERPLEXITY_API_KEY")
"""
STIMA_KEY = os.getenv("STIMA_API_KEY")


# ---------- 1. æ¨¡å‹å°ç…§è¡¨ ----------
# ä½¿ç”¨è€…ä¸‹æ‹‰çœ‹åˆ°çš„åç¨±  â†’  (provider, çœŸæ­£çš„ API æ¨¡å‹ ID)
"""
MODELS = {
    "DeepSeek-R1"               : ("openrouter", "deepseek/deepseek-r1:free"),
    "Gemini 2.0 Flash"          : ("openrouter", "google/gemini-2.0-flash-exp:free"),
    "Qwen2.5-VL-72B"            : ("openrouter", "qwen/qwen2.5-vl-72b-instruct:free"),
    "Mistral-Small-24B"         : ("openrouter", "mistralai/mistral-small-24b-instruct-2501:free"),
    "Dolphin-3.0-Mistral-24B"   : ("openrouter", "cognitivecomputations/dolphin3.0-r1-mistral-24b:free"),
    "Llama-3-70B-Instruct"      : ("openrouter", "meta-llama/llama-3.3-70b-instruct:free"),
    "GPT-4o-mini"               : ("openai",     "gpt-4o-mini"),
    "Sonar-Deep-Research"       : ("perplexity", "sonar-deep-research"),
    "Sonar-Reasoning-Pro"       : ("perplexity", "sonar-reasoning-pro"),
    "Sonar-Reasoning"           : ("perplexity", "sonar-reasoning"),
    "Sonar-Pro"                 : ("perplexity", "sonar-pro"),
    "Sonar"                     : ("perplexity", "sonar"),
    "r1-1776"                   : ("perplexity", "r1-1776")
}
"""

# ---------- 2. å»ºç«‹å°æ‡‰ Provider çš„ Async å®¢æˆ¶ç«¯ ----------
def get_client(provider):
    if provider == "openai":
        return openai.AsyncOpenAI(api_key=OA_KEY)    # ä½¿ç”¨å®˜æ–¹ç«¯é», ä¸ç”¨æ”¹ base_url
    elif provider == "openrouter":
        return openai.AsyncOpenAI(
            api_key=OR_KEY,
            base_url="https://openrouter.ai/api/v1"    # å…¶ä»–å…¨éƒ¨èµ° OpenRouter
        )
    elif provider == "stima":
        return openai.AsyncOpenAI(
            api_key=STIMA_KEY,
            base_url="https://api.stima.tech/v1"
        )
    elif provider == "perplexity":
        return httpx.AsyncClient(
            base_url="https://api.perplexity.ai",
            headers={"Authorization": f"Bearer {PP_KEY}"}
        )

# ---------- 3. å‘¼å«ä¸€æ¬¡æ¨¡å‹åšæ®µè½å­—å¥æ”¹å¯« ----------
async def rewrite_once(model_key, text, system_prompt, temp):
    provider, full_id = MODELS[model_key]
    client  = get_client(provider)
    if provider in ["openai", "openrouter"]:
        resp = await client.chat.completions.create(
            model=full_id,
            messages=[
                {"role": "system", "content": system_prompt or
                    "You are a researcher."},
                {"role": "user", "content": text}
            ],
            temperature = temp,    # ç”¨ UI å‚³é€²ä¾†çš„æº«åº¦
        )
        return resp.choices[0].message.content.strip()
    elif provider == "perplexity":
        # å‡è¨­ Perplexity Chat API é•·é€™æ¨£ï¼Œå¯¦éš›ä¾å®˜æ–¹èª¿æ•´
        payload = {
            "model": full_id,
            "messages": [
                {"role": "system", "content": system_prompt or
                    "You are a researcher."},
                {"role": "user", "content": text}
            ],
            "temperature": temp
        }
        # Perplexity å›å‚³ JSONï¼Œæ ¼å¼ä¹Ÿè¨±æ˜¯ { "choices": [ { "message": { "content": "..." } } ] }
        r = await client.post(
            "/chat/completions", 
            json=payload,
            timeout=httpx.Timeout(600.0, connect=10.0)
        )
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["message"]["content"].strip()

    else:
        raise ValueError(f"Unknown provider: {provider}")

# ---------- 4. ä¸‰æ¨¡å‹ä¸¦è¡Œæ”¹å¯« ----------
def rewrite_batch(text, model1, model2, model3, sys_prompt, temp):
    async def _run():
        return await asyncio.gather(    # gather è®“ä¸‰å€‹ coroutines åŒæ™‚åŸ·è¡Œ
            rewrite_once(model1, text, sys_prompt, temp),
            rewrite_once(model2, text, sys_prompt, temp),
            rewrite_once(model3, text, sys_prompt, temp),
        )
    return asyncio.run(_run())    # æŠŠ async çµæœè®ŠåŒæ­¥ (Gradio éœ€è¦)

# ---------- 5. Gradio ä»‹é¢ ----------
model_list = list(MODELS.keys())    # çµ¦ Dropdown ç”¨

with gr.Blocks(theme=gr.themes.Soft(), title="Chat-API") as demo:
    gr.Markdown("### ğŸ“ ä¸€æ¬¡æ¯”è¼ƒä¸‰ç¨®æ¨¡å‹çš„è¼¸å‡ºçµæœ")

    src = gr.Textbox(label="è¼¸å…¥", lines=8, placeholder="è²¼ä¸Šæˆ–è¼¸å…¥è¦æ”¹å¯«çš„å…§å®¹")
    sys_prompt = gr.Textbox(label="è‡ªè¨‚ç³»çµ±æç¤º (å¯ç©ºç™½)", placeholder="ä¾‹ï¼šä¿æŒå°ˆæ¥­ä¸”å…·æœ‰ç§‘æŠ€æ„Ÿçš„èªæ°£")

    temp = gr.Slider(0.0, 1.0, value=0.7,
                     step=0.05, label="Temperature")    # æ–°å¢æº«åº¦æ»‘æ¡¿ï¼š0.0 ~ 1.0ï¼Œé è¨­ 0.7
    
    with gr.Row():
        dd1 = gr.Dropdown(model_list, value=model_list[0],  label="æ¨¡å‹ 1")    # é è¨­ DeepSeek-R1
        dd2 = gr.Dropdown(model_list, value=model_list[1],  label="æ¨¡å‹ 2")    # é è¨­ Gemini 2.0 Flash
        dd3 = gr.Dropdown(model_list, value=model_list[12], label="æ¨¡å‹ 3")    # é è¨­ GPT-4o-mini

    btn = gr.Button("ğŸŒŸæŸ¥è©¢")

    with gr.Row():
        out1 = gr.Textbox(label="æ¨¡å‹ 1 è¼¸å‡º", lines=20)
        out2 = gr.Textbox(label="æ¨¡å‹ 2 è¼¸å‡º", lines=20)
        out3 = gr.Textbox(label="æ¨¡å‹ 3 è¼¸å‡º", lines=20)

    btn.click(
        rewrite_batch,
        inputs=[src, dd1, dd2, dd3, sys_prompt, temp],
        outputs=[out1, out2, out3]
    )

if __name__ == "__main__":
    demo.launch()
